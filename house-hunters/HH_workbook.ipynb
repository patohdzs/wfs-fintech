{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\" \" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hAXcHkODX2k"
   },
   "source": [
    "# House Hunters: Predicting Real Estate Prices using Machine Learning in Python\n",
    "### **WORKBOOK**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTqPcJlw3MRw"
   },
   "source": [
    "## External dependencies - DO NOT MODIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tys1yGct3JKc"
   },
   "outputs": [],
   "source": [
    "training_data_file_url = 'https://raw.githubusercontent.com/patohdzs/wfs-fintech-workshops/main/house-hunters/training_dataset.csv'\n",
    "test_data_file_url = 'https://raw.githubusercontent.com/patohdzs/wfs-fintech-workshops/main/house-hunters/test_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsk89YQM2rZh"
   },
   "source": [
    "## Notebook Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHRwFXcf25SO"
   },
   "source": [
    "\n",
    "*   The notebook is divided into various sections, separated by headers in a larger font size.\n",
    "*   Each section will have text describing what is being done, hopefully serving both as useful instruction of what to code but also which modelling stage we are incorporating and why.\n",
    "---\n",
    "* **Instructions on the tasks will appear separated and in bold font at the end of each piece of text, and the corresponding solution should be written in a cell directly below.**\n",
    "\n",
    "*   **Sometimes code will already be written in this cell either as an example or to carry out tasks that go beyond the scope of this workshop. In these cases, you should add your code in between the lines commented `#### ADD YOUR CODE HERE ####` and `##############`.**\n",
    "\n",
    "*   **Exercise: try printing out \"Hello World\" in the cell below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0hayeFZ22fB",
    "outputId": "cfe4605d-cb3b-46bc-94a6-40bd0751583e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "#### ADD YOUR CODE BELOW ####\n",
    "\n",
    "#############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIWeNsCbFTaG"
   },
   "source": [
    "## Step 1: Importing external modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKnbDKNA7mY3"
   },
   "source": [
    "When coding an ML project, we will use a few external libraries within our code. We do this as these libraries contain code for standard operations that we will have to do repeatedly, so it is time-efficient and easier to just re-use this code instead of trying to \"re-invent the wheel\". In this project, we will use the following libraries:\n",
    "\n",
    "- Pandas and NumPy:\n",
    "  - Used for matrix and vector manipulation. Necesary since all of our data is in the form of these objects.\n",
    "- Seaborn and Matplotlib:\n",
    "  - Used for producing visualizations, graphs and charts.\n",
    "- Scikit Learn:\n",
    "  - Main Python machine learning library!\n",
    "  - Contains methods that build, fit, predict, and assess ML models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxoWRxXdDW2U"
   },
   "outputs": [],
   "source": [
    "# Importing NumPy and Pandas libraries - matrix and vector manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importing Matplotlib and Seaborn - graphics and visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing Scikit Learn modules - Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74FWT_RJHIoF"
   },
   "source": [
    "## Step 2: Download datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgYVwe54oKpR"
   },
   "source": [
    "\n",
    "\n",
    "*   The first thing we need to do before we build our ML model is to download our datasets.\n",
    "*   We can do this quite easily if they are in `.csv` format files (most spreadsheet type files can be converted into `.csv`)\n",
    "*   We download files using the `.read_csv(filepath)` function from the Pandas library:\n",
    "  *   Takes in the filepath as an argument.\n",
    "  *   Outputs the dataset as a Pandas DataFrame, which is what we usually want for ML.\n",
    "* The above has been done to download our training dataset in the cell below.\n",
    "\n",
    "---\n",
    "\n",
    "*   **The filepath for our test dataset is already saved under the variable** `test_data_file_url` **. Try downloading this dataset in the cell below and assign it to variable** `test_data` **.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "sO2rBNK6rf-2",
    "outputId": "1eef0d81-3fb2-48a8-9a33-819af4cb6148"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>Exterior1st</th>\n",
       "      <th>Exterior2nd</th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>Foundation</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>Heating</th>\n",
       "      <th>...</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>Electrical</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Functional</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>706</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>GasA</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>8</td>\n",
       "      <td>Typ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>548</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>978</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>GasA</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>162.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Mn</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>486</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>GasA</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Wd Sdng</td>\n",
       "      <td>Wd Shng</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>BrkTil</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>No</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>216</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>GasA</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>7</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>3</td>\n",
       "      <td>642</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Av</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>655</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>GasA</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>9</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>3</td>\n",
       "      <td>836</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice\n",
       "0   1          60       RL  ...        WD         Normal    208500\n",
       "1   2          20       RL  ...        WD         Normal    181500\n",
       "2   3          60       RL  ...        WD         Normal    223500\n",
       "3   4          70       RL  ...        WD        Abnorml    140000\n",
       "4   5          60       RL  ...        WD         Normal    250000\n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(training_data_file_url)\n",
    "\n",
    "#### ADD YOUR CODE HERE ####\n",
    "\n",
    "############################\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSliztUyHd0C"
   },
   "source": [
    "## Step 3: Construct target vector and feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A91PuezS7LU5"
   },
   "source": [
    "\n",
    "\n",
    "*   The next step after downloading our datasets is to construct a *feature matrix* and a *target vector*.\n",
    "  *  Feature matrix: subset of our whole dataset containing only variables that we want to model as explanatory variables. Variables are represented as columns while each row represents an observations.\n",
    "  *   Target vector: single vector containing data for the variable we are interested in predicting.\n",
    "  \n",
    "*   To construct the above objects all we need to do is select specific columns  from our dataset:\n",
    "  1.   Create a list containing the variable names of explanatory variables\n",
    "      *   Not necessary for target vector\n",
    "  2.   Index your dataset to select only these columns\n",
    "      *   eg.// `dataset[variable_names_list]`\n",
    "      *   For target vector: `dataset[target_variable_name]`\n",
    "  3.   Save the selected samples into new variables\n",
    "      *   Conventionally, we use `X` for the feature matrix and `y` for target vector\n",
    "---\n",
    "*   **Try creating a feature matrix and target vector for our dataset using the variables below. Assign these to** `X `**and**` y ` **respectively.**\n",
    "*   **Explanatory variables:** `'LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd'`\n",
    "\n",
    "*   **Target variable:** `'SalePrice'`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0HDhofIr-9-"
   },
   "outputs": [],
   "source": [
    "#### ADD CODE HERE ####\n",
    "\n",
    "# Define features for our model\n",
    "\n",
    "# Create feature matrix\n",
    "\n",
    "# Create target vector\n",
    "\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU_gqb6kz6T0"
   },
   "source": [
    "\n",
    "\n",
    "*   After we've created our feature matrix and target vector, we're gonna want to split these into training and validation samples.\n",
    "\n",
    "\n",
    "![train_val.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "  *   The training sample consists of observations that we use to fit our model\n",
    "  *   The validation sample consists of observations which we use to asses how good our model is at predicting *out of sample*\n",
    "\n",
    "*   We want roughly 80%-70% of our data to be used in the training sample, but this varies depending on our dataset.\n",
    "\n",
    "*   To split our dataset into the two samples we use the train-test split function:\n",
    "  *   `X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=(???), random_state=(???))`\n",
    "  *   Here, we need to pass two arguments:\n",
    "      *   `test_size`: a number from 0 to 1 specifying the desired proportion of observations that go into the validation sample\n",
    "      *   `random_state`: an integer from 0 to 42 (we don't care which one, just pass one) that guarrantees that data is always split the same way. This allows our results to be *reproducible*.\n",
    "---\n",
    "*   **Try creating training and validation samples using the instructions above and the feature matrix and target vector you created previously.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4vF5ZT9yoBA"
   },
   "outputs": [],
   "source": [
    "#### ADD CODE HERE ####\n",
    "\n",
    "# Split dataset into training and validation sample\n",
    "\n",
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb5InSouIKdv"
   },
   "source": [
    "## Step 4: Construct and fit our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WNQePAD6Xeg"
   },
   "source": [
    "\n",
    "\n",
    "*   Now that we have our data samples ready, its time to build our models.\n",
    "*   To do this, we first instantiate these as objects using classes from Scikit Learn and assign them to a variable. \n",
    "  *   The constructor methods for linear regression and decision tree models in Scikit are `LinearRegression()` and `DecisionTreeRegressor` respectively.\n",
    "*   Then, we call the `.fit(X,y)` method, passing in our training sample as arguments.\n",
    "*   The above has been done for the decision tree model below.\n",
    "  *   For this model we also passed an additional `random_state` argument to ensure that the same tree is built every time, but this won't be necessary for most regression models.\n",
    "---\n",
    "*   **Try instantiating and fitting the linear regression model in the cell below. When instantiating it, assign it to variable** `lr`**.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "to5nklD9ITfj",
    "outputId": "1f0329e0-97ee-45ae-b3e5-efa455889dc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating and fitting Decision Tree model\n",
    "dt = DecisionTreeRegressor(random_state = 1)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#### ADD YOUR CODE HERE ####\n",
    "\n",
    "# Instantiating and fitting Linear Regression model\n",
    "\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5N2sSS0DnB4"
   },
   "source": [
    "## Step 5: Validate our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELew1Cfre-MX"
   },
   "source": [
    "*   To predict our target vector for some sample of observations using our model, we use the `.predict(X)` method on the desired model, passing as an argument the feature data of our sample.\n",
    "*   This is shown in the cell below for the linear regression model.\n",
    "\n",
    "---\n",
    "*   **Try predicting the y values for observations in the `X_train` sample using the decision tree model, and assign the results to variable `y_pred_dt`.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DX2RTn5fHQHk"
   },
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_val)\n",
    "\n",
    "#### ADD YOUR CODE HERE ####\n",
    "\n",
    "############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AcUvhr1mmYx"
   },
   "source": [
    "*    After we are done predicting, we need some way of assesing how good our predictions are. This process is called *validation*.\n",
    "*    The validation metric used for this model is the Root-Mean Squared Error of predictions: $$\\sqrt{  \\frac{ \\sum_{i=1}^{n} (y_i-\\hat{y_i})^2 }{n}\\ } $$ Where $\\hat{y_i}$ is the predicted value and $y_i$ the true value for the $i$th observation, and $n$ is the number of observations.\n",
    "\n",
    "*   To compute this, we use the function `mean_squared_error(y_true, y_pred)`, assign it to some variable and then just take its square root.\n",
    "*   This has been done and is shown in the cell below for the linear regression model.\n",
    "---\n",
    "*   **Try computing the RMSE for the decision tree model. Assign it to variable `mse_dt`**\n",
    "*   **Print out the RMSE for the decision tree model. Right now, which seems to be a better model? LR or DT?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkQwPW6res5-",
    "outputId": "f5b9416b-7cae-4cc2-f93b-f7dae862e8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Linear Regression: $ 40873.73\n",
      "RMSE Decision Tree: $ 46729.33\n"
     ]
    }
   ],
   "source": [
    "# Computing RMSE for linear regression predictions\n",
    "rmse_lr = mean_squared_error(y_val, y_pred_lr)\n",
    "rmse_lr = round(rmse_lr**0.5,2)\n",
    "print('RMSE Linear Regression: $', rmse_lr)\n",
    "\n",
    "#### ADD YOUR CODE HERE ####\n",
    "\n",
    "# Computing RMSE for decision tree predictions\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfl0xL3dqZrv"
   },
   "source": [
    "## Step 6: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u4oY35bVG-y"
   },
   "source": [
    "\n",
    "\n",
    "*   Now that we know how to build, predict, and validate models, we will focus on how to tune them to improve their performance.\n",
    "*   Steps for hyperparameter tuning:\n",
    "\n",
    "  1.   Choose a hyperparameter.\n",
    "  2.   Create a list containing different possible arguments for this hyperparamter.\n",
    "  3. Iterate through this list. At each iteration, build and validate an instance of your model passing in the corresponding hyperparameter argument.\n",
    "  4. Assess the validations to determine which hyperparameter argument improves predictive performance the most\n",
    "\n",
    "---\n",
    "*   **Declare a list with the following possible arguments for the** `max_leaf_nodes` **parameter:** `5, 25, 50, 100, 250, 500` **.**\n",
    "*   **Use a 'for' loop to iterate through this list. At each iteration, call the function** `get_rmse` **which will build and validate a model with the hyperparameter being tested. Assign this result to variable** `rmse` **.**\n",
    "\n",
    "* **At each iteration, print out the RMSE (stored as variable** `rmse` **) and the hyperparameter argument tested.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfQQkJ5yIZ3L",
    "outputId": "772032bd-eae8-40a3-f6cd-f154d8de4447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_leaf_nodes:  5\n",
      "RMSE: $ 50830.83\n",
      "\n",
      "max_leaf_nodes:  25\n",
      "RMSE: $ 43046.78\n",
      "\n",
      "max_leaf_nodes:  50\n",
      "RMSE: $ 43152.99\n",
      "\n",
      "max_leaf_nodes:  100\n",
      "RMSE: $ 43759.58\n",
      "\n",
      "max_leaf_nodes:  250\n",
      "RMSE: $ 44536.45\n",
      "\n",
      "max_leaf_nodes:  500\n",
      "RMSE: $ 45337.72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_rmse(max_leaf_nodes, X_train, X_val, y_train, y_val):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds_val = model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_preds_val)\n",
    "    rmse = round(rmse**0.5,2)\n",
    "    return(rmse)\n",
    "\n",
    "#### ADD CODE HERE ####\n",
    "\n",
    "# Tuning hyperparameter 'max_leaf_nodes'\n",
    "\n",
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIhuBuwmD8y9"
   },
   "source": [
    "## Step 7: Build the final models and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvE2My-fqUJe"
   },
   "source": [
    "\n",
    "*   For the final stage of our modelling, we will now build tuned models and use them to predict on *unlabeled data*.\n",
    "*   We will train these models with our complete dataset (training sample + validation sample).\n",
    "*   We will use hyperparameter arguments that improved performance the most.\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**1.   Create a feature matrix for the test sample** `test_data` **using the features selected in Step 3. Assign it to variable** `X_test` **.**\n",
    "\n",
    "**2.   Instantiate our two final linear regression and decision tree models, and remember to pass our optimal hyperparameters when doing this. Assign these to variables** `final_lr` **and** `final_dt` **respectively.**\n",
    "\n",
    "**3.   Fit these models using our complete labeled dataset, which is stored under variables** `X` **and** `y` **for the feature matrix and target vector respectively.**\n",
    "\n",
    "**4.   Use our fitted models to predict y-values for our** `X_test` **sample. Assign the predictions to variables** `y_pred_final_lr` **and** `y_pred_final_dt` **for linear regression and decision tree final models respectively**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu6j0JbpHdDe"
   },
   "outputs": [],
   "source": [
    "#### ADD CODE HERE ####\n",
    "\n",
    "# Create test feature matrix\n",
    "\n",
    "# Instantiate final models\n",
    "\n",
    "# Fit final models\n",
    "\n",
    "# Predict using our final models\n",
    "\n",
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-TCEsByEGVF"
   },
   "source": [
    "## Step 8 (Optional): Submit your predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlKB5EsV1M1d"
   },
   "source": [
    "\n",
    "\n",
    "*   In this section, we wrote some code that turns your predictions into `.csv` files.\n",
    "*   These files are suitable submissions to the Kaggle Housing Prices competition. Kaggle is a mecca for datasets, ML competitions, Notebooks and anything data-related.\n",
    "*   To submit your predictions click on [this link](https://www.kaggle.com/c/home-data-for-ml-course), create a Kaggle profile and follow the isntructions in the website. Contact us if you need any assistance in the process.\n",
    "\n",
    "*  After submitting, you will be able to see how your predictions stack up against other competitors in the leaderboard. We recommend you then spend some time working on this model to improve your score. The next section includes some suggestions on how you may go about this.\n",
    "---\n",
    "*   **If you want to create submission files, uncomment lines 3 and 7 in the cell below.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtWNPOQS00qz"
   },
   "outputs": [],
   "source": [
    "# Creating submission file for linear regression predictions\n",
    "output = pd.DataFrame({'Id': test_data.Id,'SalePrice': y_pred_final_lr})\n",
    "#output.to_csv('submission_lr.csv', index=False)\n",
    "\n",
    "# Creating submission file for decision tree predictions\n",
    "output = pd.DataFrame({'Id': test_data.Id,'SalePrice': y_pred_final_dt})\n",
    "#output.to_csv('submission_dt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XNHDwNCE66c"
   },
   "source": [
    "## Step 9 (Optional): Tips to improve your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xbk_RkbZFJnZ"
   },
   "source": [
    "\n",
    "\n",
    "1.   Try including more features\n",
    "  - How do we treat categorical vs continuous features?\n",
    "  -  Does more features always $\\implies$ better model?\n",
    "  - Curse of dimensionality\n",
    "  \n",
    "2.   Try tuning other hyper-parameters\n",
    "  - Using multiple sample cross-validation\n",
    "  - Using grid-search to cross-validate\n",
    "3.   Try using different (more advanced) models\n",
    "  - LAST THING TO CHANGE!!!!!!\n",
    "  - Regularized regression: Ridge, Lasso, Elastic Net\n",
    "  - Support vector machines: SVRegressor\n",
    "  - Tree-based models: Random Forest\n",
    "  - Holy Grail: XGBoost Regressor\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
